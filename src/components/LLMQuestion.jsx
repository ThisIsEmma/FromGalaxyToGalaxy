import React from 'react';
import '../styles/LLMQuestion.css';

const LLMQuestion = () => {
  return (
    <div className="context-page">
      <h1>ğŸ§  My Evolving Question About Meaning in LLMs</h1>
      <p className="intro-quote">
        â€œI used to think language models only predict the most common next words based on huge datasets â€”
        like after the word â€˜itâ€™, probably comes â€˜isâ€™ or â€˜wasâ€™.
        But now Iâ€™m learning that the model can actually figure out what â€˜itâ€™ refers to in a sentence, like 'the mouse'.
        <br /><br />
        So my question is: <strong>How does a model trained on huge general data actually focus on the specific meaning of â€˜itâ€™</strong>
        in a sentence like this?
        <br />
        Does it still rely on general next-word patterns?
        <br />
        Or is it truly understanding context â€” like humans do, when we infer that â€˜itâ€™ = â€˜mouseâ€™?â€
      </p>

      <h2>ğŸ” Youâ€™re Comparing Two Views:</h2>
      <table className="comparison-table">
        <thead>
          <tr>
            <th>View A: Pattern Memory (shallow)</th>
            <th>View B: Contextual Understanding (deep)</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>â€œItâ€ is followed by â€œrains,â€ â€œis,â€ â€œwasâ€ etc.</td>
            <td>â€œItâ€ refers to something earlier in the sentence (â€œmouseâ€)</td>
          </tr>
          <tr>
            <td>The model just learns what words follow what</td>
            <td>The model builds an understanding of what â€œitâ€ means</td>
          </tr>
          <tr>
            <td>Language = autocomplete on steroids</td>
            <td>Language = relational, structured meaning</td>
          </tr>
          <tr>
            <td>Based on stats over the whole internet</td>
            <td>Based on interpreting this exact sentence right now</td>
          </tr>
        </tbody>
      </table>

      <section className="context-section">
        <h2>ğŸ¯ So which is correct?</h2>
        <p>âœ… Both are true â€” but the second is deeper. This is what modern Transformers like GPT-4 actually do.</p>

        <h3>ğŸ”¹ First, Yes: Early models were more like View A.</h3>
        <p>They just tracked word patterns â€” like saying â€œafter â€˜itâ€™ usually comes â€˜isâ€™ or â€˜was.â€™â€</p>
        <p>But they couldnâ€™t understand what â€œitâ€ refers to.</p>

        <h3>ğŸ”¹ But Transformers do more: View B</h3>
        <p>Modern LLMs build meaning using self-attention and huge data.</p>

        <h3>ğŸ’¡ How?</h3>
        <ul>
          <li>ğŸ‘€ They look at the whole sentence</li>
          <li>ğŸ” Compute attention for each token</li>
          <li>ğŸ§  Combine those attentions to choose what â€œitâ€ means</li>
        </ul>

        <p>Example: â€œThe cat chased the mouse. It ran away.â€ â†’ It = the mouse.</p>

        <h3>ğŸ§  Analogy: How humans do it</h3>
        <p>We donâ€™t guess what follows â€œitâ€ â€” we infer what â€œitâ€ *is* based on sentence logic.</p>

        <h3>âœ… So your question: â€œHow does it know â€˜itâ€™ refers to mouse?â€</h3>
        <p>It learns through training on examples like â€œThe dog chased the cat. It ranâ€¦â€</p>
        <ul>
          <li>âœ” It discovers reference patterns</li>
          <li>âœ” Learns to attend to subjects/verbs/nouns</li>
          <li>âœ” Builds meaning via self-supervised learning</li>
        </ul>

        <h3>ğŸ›  Who builds this understanding?</h3>
        <ul>
          <li>Humans build the architecture (Transformer)</li>
          <li>Give it an objective (predict the next word)</li>
          <li>The model *discovers* meaning on its own</li>
        </ul>

        <h2>ğŸ¯ Final Answer:</h2>
        <p>â“Is it about frequent word patterns â€” or actual meaning?</p>
        <p>âœ… Itâ€™s both â€” but models like GPT lean toward meaning.</p>
        <p>This is why <strong>self-attention</strong> is the heart of LLMs.</p>

        <div className="questions">
          <p>Would you like to:</p>
          <ul>
            <li>ğŸ§ª Simulate GPTâ€™s attention on a sentence?</li>
            <li>ğŸ“Š Visualize â€œitâ€ referring to â€œmouseâ€ in a map?</li>
            <li>ğŸ§­ Explore what â€œmeaningâ€ means in vector space?</li>
          </ul>
        </div>
      </section>
    </div>
  );
};

export default LLMQuestion;
